### 3.1:

Task 1: Real Life
- A = {sleep, work}
- S = (awake, asleep)
- P(asleep, 1 | awake, sleep) = 0.9 # going to sleep
  P(awake, -1 | awake, sleep) = 0.1 # insomnia
  P(asleep, -10 | awake, work) = 0.9 # sleeping at work
  P(awake, 2 | awake, work) = 0.1 # working is rewarding
  P(asleep, 100 | asleep, sleep) = 0.99 # going into deeper sleep
  P(awake, -1000 | askleep, sleep) = 0.01 # waking up from realizing you've tried to dream in a dream, in your bed, then starting to wonder if life is not just a dream
  P(asleep, 10 | asleep, work) = 0.8 # dreaming of work in your dream, and staying in your dream
  P(awake, -2 | asleep, work) = 0.2 # dreaming about work because late to work, so waking up unconsciously in a bad mood

Task 2: Play Chess against Random player
- A = {every valid chess move}
- S = {possible chess position. Check mate goes through terminal state.}
- P = {probability of random player going through next state, with reward of +1 if AI didn't block a check (or cannot), 0 if nothing happens and -1 if opponent captured one of your piece or won}

Task 3: Hardcore Navigation
- A = {decide if you should accelerate, brake, change angle to left or change angle to right}
- S = {image you see from the front camera} (in R^(16x16))
- P = {0 most of the time, -np.inf if accident (terminal state)}
--> Limits to the traditional MDP model:
1. States don't fully capture the actual state of the environment (i.e. you can see the same thing multiple times but be in a different position).
2. Continuous action space.
3. Infinite / continuous state space.
4. Infinite reward.

### 3.2:

It's hard to find clear exceptions because all "goal directed behaviour" can be redefined as a MDP where "goal achieved" = terminal state with transition leading to this state having a reward of 1, and rest of transitions having a reward of 0. 

However, in practice the MDP framework is not useful when the rewards are not (yet) well-defined or hard to define. For instance, as a clear exception, "understand a text" would be something hard to define for a ML model in February 2020. As for the walking robot problem, defining what "walking in a human way" or "behaving according to human values" is somehow arbitrary and hard to specify clearly.

### 3.3:

The natural place to delimit the agent and the environment is what the agent can move in less than 0.01s (e.g. our muscles). The basis is "it has actual control over that thing". For a robot that limit might be different (can send signals much quicker, but cannot move as fast). It's somewhat a free choice where you draw the line, but I still feel that "what the agent can actually move in 0.1s" is kinda natural for human agents.

### 3.4:

state s | action  | state s' | reward r | p(s', r | s, a)
--------------------------------------------------------
high | search | high | r_{search} | \alpha
high | search | r_{search} | 1-\alpha
low |search | high | -3 | 1-\beta
low | search | low  | r_{search} | \beta
high | wait | high | r_{wait} | 1
low | wait | low | r_{wait} | 1

### 3.5:

In episodic tasks, S^{+} = S \bigup {terminal state}

In (3.3), considering the terminal state as an absorbing state, we just need to replace S by S^{+}.

If we want to make sth more complex, then we might consider terminal state as sth that goes into the first state after a reset? But Suttong & Barto say "very slightly" so I guess just need to change S to S^{+}.

### 3.6:

The return would be exactly the same?

### 3.7:

Must also include a penalty for every step, so that it learns to get out as quickly as possible.

### 3.8:

G_5 = 0
G_4 = 2
G_3 = 3 + (1/2) * 2 = 4
G_2 = 6 + (1/2) * 4 = 8
G_1 = 2 + (1/2) * 8 = 6
G_0 = -1+ + (1/2)* 6 = 2

### 3.9:

G_1 = 7 / (1-0.9) = 70
G_0 = 2 + 0.9 * 70 = 65

### 3.10:

\sum_{k=0}{n} \gamma^k = (1 - \gamma^{n+1}) / (1-\gamma) (just multiply both sides by (1-\gamma). It's a sum of positive things inferior to 1/(1-\gamma) so it has a limit. Taking the limit, the sum is 1/(1-\gamma).

### 3.11:

\mathbb{E}[R_{t+1}] = sum_{r \in R} r \sum{s'} \sum{a} p(s', r|s, a) \pi(a | s)

### 3.12: 

v_{\pi}(s) = \sum{a} \pi(a|s) q_{\pi}(s, a)

### 3.13:

q_{\pi}(s, a) = \sum{s'}\sum{r} p(s', r| s, a) (r + \gamma.v_{\pi}(s'))

### 3.14:

0.7 ~= (1/4) * (0.4 - 0.4 + 2.3 + 0.7) = 3/4

### 3.15:

Intervals are important, not the sign. Using 3.8, adding a constant c adds v_c=c/(1-\gamma) to the value of all states.

### 3.16:

In maze running, if the reward for each step is not negative then the agent might want to never finish the task.

### 3.17:

q_{\pi}(s, a) = \sum_{r, s'} r + \gamma \sum_{a'} \pi(a'|s')q_{\pi}(s', a')

### 3.18:

v_{\pi}(s) = \mathbb(E)_{a~\pi(\dot|s)}[q_{\pi}(s, a)]

v_{\pi}(s) = \sum_{a} \pi(a|s) q_{\pi}(s, a)

### 3.19:

q_{\pi}(s, a) = \mathbb(E)[R_{t+1} + v_{\pi}(S_{t+1})]
              = \sum_r \sum_{s'} p(s', r|s, a) (r + \gamma.v_{\pi}(s'))

### 3.20:

optimal state-value function is: -2 everywhere apart the green (just use the driver to get to the green then putter), -1 if already on the green and 0 on the flag.

### 3.21:

-4 if cannot reach the -2 zone of q*(., driver), -3 if can reach this zone, -2 if close enough to reach the green, -1 if on the green, 0 if on the flag.

### 3.22:

\gamma=0: left
\gamma=0.5: both
\gamma=0.9: right

### 3.23:

Let mh = max(q*(h,s), q*(h,w)) and ml=max(q*(l, s), q*(l, w), q*(l, re))
let a = \alpha and b = \beta and g = \gamma

q*(h, s) = rs + g (a.mh + (1-a)ml)
q*(h, w) = rw + g.mh

q*(l, s) = b(rs + g.ml) + (1-b)(-3 + g.mh)
q*(l, w) = rw + g.ml
q*(l, re) = 1 + g.mh

### 3.24:

### 3.25:

### 3.26:

### 3.27:

### 3.28:

### 3.29:
