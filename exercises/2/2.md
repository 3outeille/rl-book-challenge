### 2.1: Epsilon Greedy

If \epsilon is 0.5 than probability of the greedy action is 0.5.

### 2.2: Bandit Example

It definitely occured at the timesteps 4 and 5 (at timestep 4 because Q(2) < 0 = Q(3), and at timestep 5 because Q(2) > Q(3) = 0).

### 2.3:

The \eps=0.01 will be better in the long run. It will select the optimal action 11=(0.99/0.9)x times more often. The EV is given by: EV = (1-\epsilon) * mean_maximum_action + \epsilon * EV(random action) = (1-\epsilon) * mean_maximum_action because the action values were sampled from a gaussian of mean 0.

Therefore, the solution with \epsilon = 0.01 will have (0.99 - 0.9) * mean_maximum_action = 0.09 * mean_maximum_action more reward per action.

### 2.4:

Q_{n + 1} = Q_1 \prod_{i=1}^n (1-\alpha_i) + \sum_{i=1}^n \alpha_i \prod_{k=1}^{n-i} (1 - \alpha_{n-i-k+2})


### 2.5:

Main result is that reward get up to 2+ with alpha=0.1 but if we do sample average it only goes up to 1.5. From plotting the distance to true q, we see that we sample average at some point we get further and further but with a constant alpha we keep on getting closer.

### 2.6: Mysterious spikes

The optimistic greedy will underperform for the first moves or so, because it will try out the arms that have initial values of 5. However, there will be some kind of spike (on average after 5 moves) because it will definitely find the best value in the first 10 steps, on average after 5 moves. Then, after 10 moves, it tried out all the arms at least once, so will start to play a bit with the arms greedily, being a bit better than the one with the realistic algorithm because already explored all of them. So it will grow faster (in percentage of optimal action) until it outperforms the realistic eps-greedy.

### 2.7:  Unbiased constant step-size trick

The value in front of Q_1 is multiplied by 1-\beta_1 right at the beginning with \beta_1 = 1 so no bias.

More generally, \beta_n = \frac{\alpha}{1-(1-\alpha)^n}, which tends to \alpha when n->+inf (constant step size).


Replacing the \alpha_i by the value of \beta_i and using some cool product trick where everything simplifies, we get that:

the sum of weights is: \sum_{i=1}^n \frac{1-\beta}{1 - \beta^i} \beta^{n-i} \frac{1-\beta}{1 - \beta^{n-i+1}} where \beta = (1-\alpha).

It does sum to 1 (miraculously) for n=1 and n=2.

And for the exponentially weighted, the \beta^{n-i} makes it exponentially smaller for earlier rewards (i=1) than for the latest reward (n=i).

EDIT: after having coded the function (cf. numpy/2/weights.py) it occurs that it doesn't really sum to 1... BUT the sum does converge to some mysterious number. So it's still exponentially weighted with the total weight being fixed at some point.
