### 4.1:

q_{\pi}(11, down) = -1
q_{\pi}(7, down) = -15

### 4.2:

v_{\pi}(15) = -19 then -20

### 4.3:

(4.3) q_{\pi}(s, a) = \mathbb(E)[R_{t+1} + \gamma.q_{\pi}(S_{t+1}, A_{t+1}) | S_t=s, A_t=a]
(4.4) q_{\pi}(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma.\sum_{a'} \pi(a'|s').q_{\pi}(s', a')]
(4.5) q_{k+1}(s, a) = \sum_{s', r} p(s', r | s, a) [r + \gamma.\sum_{a'} \pi(a'|s').q_(s', a')]

### 4.4:

my solution: keep a list of past policies and stop when looping on past policy (see numpy/4/figures)

### 4.5:

cf. code in numpy/4/figures

### 4.6:

intuitive needed changes:
- in 3), make the argmax action have value \pi(a|s) of (|min(A(s))|-\epsilon)/|min(A(s))|, \epsilon/|min(A(s))|.
- in 2), make the update be a stochastic sum with \pi(a|s)
- in 1), make all the values be above |min(A(s))|
