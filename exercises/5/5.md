### 5.1:

the value function jumps off because it's highly likely to win if you stick with 20 (and you definitetly win or draw if you get 21), but if you hit with something like 19 you're very likely to lose.

it drops off for the last row on the left because if the dealer has an ace than it's hard to win.

the frontemost values are higher in the upper diagrams because a sum of 12 is alright when you have a usable ace, but pretty bad otherwise.

### 5.2

because we suppose we're getting cards from an infinite deck (i.e. with replacement), it doesn't seem to really matter if we use every-visit or first-visit MC.

### 5.3

root is the state-action pair, ending in a terminal state (which is not a state-action pair).

### 5.4

Q_{n+1}(s, a) = Q_{n}(s, a) + (1/n) (R_n - Q_{n}(s, a)

cf. implementatino in MonteCarlo ES of numpy/5/mc.py

### 5.5

in this case the importance sampling ratio is 1 (only one action so b=\pi anyway), so the two alternatives (weighted importance sampling and ordinary importance sampling) are equivalent. G_t is 10, so for first visit V(s) = 10 and every-visit V(s) = 10/10 = 1.

### 5.6

same with Q(s,a), just change T(s) with T(s, a), i.e. visits to the state-action pair.

### 5.7

for the weighted importance-sampling method, the estimates are biased so the few first biases make the error be more and more wrong? because we're more likely to see highly biased things? but then if we average more we get better estimates?

### 5.8

intuitively, if we divided by the number of visits (as in every-visit), then we might in the worst case get 1/k if the legnth of the episode is k, which would give 0.2 * \sum_k (1.8^k / k) which still converges up to infinity (bc larger than an harmonic sum)?

### 5.9

code in numpy/5/mc.py class MonteCarloFirstVisit

### 5.10

here's how the derivation goes:

1) split upper sum in two: WnGn and sum_k WkGk up to n-1

2) write the lower part as Cn

3) for the right part, it's actually Cn-1/Cn times Vn

4) write Cn-1 as Cn - Wn

5) put stuff together and you get the result

### 5.11

my intuition: because we stop when \pi(S_t) is not A_t, and ties are broken consistently, then \pi(A_t|S_t) is either 1 or 0. if it's 0 we already stopped and if it's 1 then 1/b(A_t|S_t) is correct. 

### 5.12

left term is equal to sum_s sum_a b(a|s) \sum_{s',r} p(s', r | s, a) \sum_a' b(a' | s') ... \sum_a^(T-1) x (product \frac{\pi(a^(i)|s^(i))}{b(a^(i)|s^(i))}) x r

all of the below b can be simplified one by one with the b from taking the expected value. and then what's left at each step is the sum over pi(a^(i)|s^(i)) (equal to 1) and then the sum over the transitions (p(s^(i+1), r^(i) | s^(i), a^(i))) is also 1.

we're left with \sum_s \sum_a \pi(a|s) \sum_{s',r} p(s', r | s, a) r

or equivently, \sum_s sum_a b(a|s) \sum_{s', r} \frac{\pi(a|s)}{b(a|s)}

i.e. the right term.

### 5.13

the formula for V(s) is the same as for Q(s,a), just need to change T(s) as T(s,a) or time of visits to pair (s,a).

then it's the same formula with \sum (W_k * G_k) / \sum W_k, we just need to change G_k with G_{t:h} and the weights for k are (1-\gamma) \gamma^k \ro_{t:h-1} for k <= T - t - 2, and \gamma^{T-t-1} \ro_{T-1} for k = T-t-1.

actually, now the weights depend both on t and h so it's sum of W(s,h) G(s,h) divided by a sum of W(s,h).

for the details, see numpy/5/mc.py OffPolicyMCControl then truncated_weighted_avg_est.
