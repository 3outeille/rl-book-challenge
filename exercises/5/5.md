### 5.1:

the value function jumps off because it's highly likely to win if you stick with 20 (and you definitetly win or draw if you get 21), but if you hit with something like 19 you're very likely to lose.

it drops off for the last row on the left because if the dealer has an ace than it's hard to win.

the frontemost values are higher in the upper diagrams because a sum of 12 is alright when you have a usable ace, but pretty bad otherwise.

### 5.2

because we suppose we're getting cards from an infinite deck (i.e. with replacement), it doesn't seem to really matter if we use every-visit or first-visit MC.

### 5.3

root is the state-action pair, ending in a terminal state (which is not a state-action pair).

### 5.4

Q_{n+1}(s, a) = Q_{n}(s, a) + (1/n) (R_n - Q_{n}(s, a)

cf. implementatino in MonteCarlo ES of numpy/5/mc.py

### 5.5

in this case the importance sampling ratio is 1 (only one action so b=\pi anyway), so the two alternatives (weighted importance sampling and ordinary importance sampling) are equivalent. G_t is 10, so for first visit V(s) = 10 and every-visit V(s) = 10/10 = 1.

### 5.6

same with Q(s,a), just change T(s) with T(s, a), i.e. visits to the state-action pair.

### 5.7

for the weighted importance-sampling method, the estimates are biased so the few first biases make the error be more and more wrong? because we're more likely to see highly biased things? but then if we average more we get better estimates?

### 5.8

intuitively, if we divided by the number of visits (as in every-visit), then we might in the worst case get 1/k if the legnth of the episode is k, which would give 0.2 * \sum_k (1.8^k / k) which still converges up to infinity (bc larger than an harmonic sum)?
