### 6.1:

G_t - V_t[S_t] = sum_{t}^{T-1} \gamma^{k-t} (\delta_k + \gamma.(V_{k+1}[S_{k+1}] - V_k[S_{k+1}])

so the diff. G_t - V_t[S_t] - (sum of td errors) is inferior to (\gamma / (1 - \gamma)) * ||V_{k+1} - V_k||_{inf}

### 6.2:

considering the driving pb., a situation where TD update might be more efficient than MC update: when i can't wait until the end of the episode to make my prediction (like, online learning for an autonomous vehicle on unknown environment). or when the MC trajectories are too difficult to sample, as they would necessit an intractable number of steps to reach the end of the episode (cf. race track example from end of chapter 5).

HINT CASE: here, with TD updates, we're updating using the correct value for "entering highway" directly, without having to simulate the entire episode. my intuition tells me that if we stay on the original scenario, then we would O(n) TD updates where only one MC update would work.

### 6.3

the first episode ended in the left absorbing state with value 0. only V[A] changed because for all the other states V[S_t] and V[S_{t+1}] were the same. here, V[Absorbing state] = 0 so the TD error was 0 + 1 * 0 - 1/2 so it diminished by -0.05 (step-size = 0.1).

### 6.4

theoretical answer:
- for MC: lower step size means slower convergence, so won't improve results. from the graph, results stop getting better at stepsize 0.3/0.4, and starts even to be counterproductive to use a higher alpha.
- for TD: idem for lowering the step size. as for using a higher step size, it makes the convergence quicker but the bias is higher (systemic error).

empirical answer (cf. plots/ex6.4.png: using higher step sizes did indeed lead to really high bias (and converging to wrong value with high variance), and using smaller step sizes is definitely too slow (doesn't converge in 100 episodes), but should give better results if we run for more episodes?

### 6.5:

theoretical answer: my inituion is that this init value of 0.5 maximizes the kind of variance in the random walks, so that there's an infinity of variance somehow in the samples we get, and so the error gets higher throughout the episodes because we get longer walks? using smaller of bigger V_init will make it converge faster toa wrong value somehow?

empirical answer (cf. plots/ex6.5.png): among other initialization (V_init = 0, 0.25, 0.75 or 1) it only happened that clearly with 0.5.

### 6.6:

first way (which i used): solve system of equations.

second way: dynamic programming.

i think the system of equations was used because, given how slow MC/TD methods are, it doesn't seem that you could get fast convergence with DP. and also we can compute exact values here, duh.
