Given two policies pi and pi' identical except that and What can you tell for policies pi and pi' ? (two things) // 1. pi' is as good or better than pi 2. if inequality is strict, pi' is in fact better than pi$
First TD formula in tic-tac-toe$
Why does the red arrow goes two steps up in the tic-tac-toe? // "Because the opponent is kind of a stochastic environment. So our RL agent is trying to predict the value of reaching ""state before stochasticity in environment""."$
How is defined the value of an action in the k-armed bandit problem?$
What's the formula for estimating the true value of an action in the k-armed bandit pb (with indicator functions)?$
What's the formula for the greedy action selection method (in k-armed bandit)?$
What's the formula for the estimated action value, indexed by n, in terms of rewards?$
How do you re-write the estimate of action value (indexed by n) to limit memory/computational requirements? (n+1 in function of n)$
What's the general update rule used in the book? (the average hack being a special case)$
How is the average called when using the formula with the step size for Q_{n+1}? // exponential recency-weighted average$
What are the conditions on the weights (from stochastic approximation theory) to assure convergence with probability 1 of the Q values using the formula with Q_{n+1} and the step size alpha?$
What's the formula for UCB?$
What's the probability \pi_t(a) of selection action a at timestep t in the Gradient Bandit Algorithm?$
What's the update rule for the preference of the selected action in the Gradient Bandit Algorithm?$
What's the update rule for the preference of the other actions in the Gradient Bandit Algorithm?$
What's the thing that we need to show to prove that we're actually doing stochastic gradient ascent? // That the expected update of the gradient bandit algorithm is equal to the gradient of expected reward.$
What would be the formula for exact gradient ascent (In the case of Gradient Bandit algorithms)?$
How do we call a task that change over time (e.g. random walk)?	nonstationary$
another name for contextual bandit // another name for associative search task$
another name for associative search task // another name for contextual bandit$
Visualize the arrow updates in the tic-tac-toe diagram$
How do you define a trajectory?$
How do you define r(s,a) and to what is it equal?$
What's the first (simple) definition for the expected return G_t?$
How do you define the discounted return G_t?$
How are returns at successive time steps related?$
How do you define the value function of a state s under \pi?$
How do you define the action-value function for policy \pi?$
Bellman equation for v_{\pi}$
How is the absorbing state defined? (two things) // 1. transitions to itself 2. rewards of zero$
Visualize the absorbing state example$
visualize the backup diagram from v_{\pi}(s) to \q_\pi(s, a)$
visualize the backup diagram from q_{\pi}(s, a) tp v_{\pi}(s')$
(formula) optimal state-value function	(name)$
(name)	(formula) optimal state-value function$
(formula) optimal action-value function	(name)$
(name)	(formula) optimal action-value function$
optimal action-value function in terms of optimal state-value function (with upper letter variables)$
Bellman optimality equation (for v*)$
Bellman optimality equation (for q*)$
visualize backup diagram for Bellman optimality equation (for v*)$
Bellman equation (for v_{\pi})$
What formula do you use for the update in iterative policy evaluation?	// Bellman equation for v_{\pi}$
Name of theorem which says that given a comparison between action value function and state value function, then we get a comparison between value functions? // policy improvement theorem$
In the dynamic programming chapter, what conditions does the greedy policy meet? // Those of the policy improvement theorem$
How do we call the process of making a new policy that improves on the original policy by making it greedy w.r.t the value function of the original policy?	// policy improvement$
Visualize policy iteration$
What are the two arguments that guarantee the convergence of policy iteration? // 1. each policy is a strict improvement over previous one (unless optimal already) 2. number of policies in a finite MDP is finite$
Why is the improvement always strict unless already optimal? // Because if not strict then satisfies Bellman equation.$
