- at page 96, it says that the _exploring starts_ hypothesis consists in that episodes start at a state-action pair and every pair has a non-zero probability of being selected at the start. how could this really happen in Atari environments or gym environments? 

-> answer: you select those action pairs artificially, cf. beginning of the Monte Carlo ES algorithm. you could like list all those states and actions by using the gym API.

- at the top of page 98, there are some inequalities with some \pi_k(s). i guess this means we're using a deterministic policy. would it also work with stochastic policies?

- what does "uniformly better" mean, page 98?
