- When considering deterministic policies, does the Bellman equation always have an unique solution, even in the un-discounted case?

- Under which condition is it the case that if v_{\pi} satisfies the Bellman equation then \pi is optimal?

- Under which condition does a MDP has a unique optimal policy? What's the proof?

- In the undiscounted gridworld example, what happens when the initial policy has loops so policy evaluation doesn't converge?

- For the car rental example
    1. why does policy iteration finishes after two iterations? Why is the correct policy (in the 4 possible actions case) [0, 0, 0, 0], [1, 1, 1, 1], [0, 0, 0, 0], [0, 0, 0, 0]?
    2. (17/03/2020) why does policy iteration seem to always converge to a policy of "not doing anything" (zero everywhere) and the same value of about 86 for all states?
    3. (19/03/2020) why does policy evaluation sometimes keeps growing, overflowing?
