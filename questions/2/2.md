How does the theorems from https://en.wikipedia.org/wiki/Stochastic_approximation apply for our case (cf. 2.7) exactly? What is the root? What is M(\theta)? What is N(\theta)? Why do we have the derivative and the non-decreasing conditions satisfied?

Why does in my plot the optimistic greedy outperforms the realistic eps-greedy after >400 moves and in Sutton it's after 180 steps?

- **(Not an) Answer**: I hadn't changed the "random_walk = True" variable. After setting it to False it's a bit less than 400.
Better Ansewr: there's a lot of variance when averaging over 100 bandits. Will try with 2000 bandits. Result: Outperforms at 800 (average of 2k bandits)... even worse!

In Figure 2.3, is the %Optimal Action given by the average reward? Or by the maximum of estimate q-values max(Q)?

- **Answer**: if the estimation is wrong then we could over-estimate. Not sure if this happens on average though. Could be always < q-value of optimal action on average. Would be more coherent with fig 2.2. to keep it as the average reward.

In Exercise 2.7, is it supposed to sum to 1, or did I do a mistake?

In Figure 2.4, why does Sutton's UCB performs slightly better but my UCB performs way better?

- **Answer**: there was a mistake bc I was using random walk for one and not the other. Then, for big c values (>=2 for instance),  the exploration factor is twice as much as for 1. So when it chooses its first action (which was pretty good), it will need to visit other less good actions (thus decreasing from the just "argmax").

In Figure 2.5, why do I get the opposite curves in function of alpha being 0.1 or 0.4?

- **Answer**: because I had forgotten to add the random walk to the variables q. Intuitively, with a random walk, the highest learning rate wins (because it can better catch up the moving values?)

In the math box from the bandit chapter:
1. Why can we replace q_{*}(A_t) by R_t "because E[R_t|A_t] = q_{*}(A_t)"? Where do we condition on A_t? How do conditional expectation work anyway?
2. What if \pi_t(a) = 0? How does the trick work then?
3. Why can we replace things by their expected value in the expected value operator?
