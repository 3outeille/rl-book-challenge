- How does the theorems from https://en.wikipedia.org/wiki/Stochastic_approximation apply for our case (cf. 2.7) exactly? What is the root? What is M(\theta)? What is N(\theta)? Why do we have the derivative and the non-decreasing conditions satisfied?

- Why does in my plot the optimistic greedy outperforms the realistic eps-greedy after >400 moves and in Sutton it's after 180 steps?

(Not an) Answer: I hadn't changed the "random_walk = True" variable. After setting it to False it's a bit less than 400.
Better Ansewr: there's a lot of variance when averaging over 100 bandits. Will try with 2000 bandits.
-> Outperforms at 800 (average of 2k bandits)... even worse!

- In Figure 2.3, is the %Optimal Action given by the average reward? Or by the maximum of estimate q-values max(Q)?

Answer: if the estimation is wrong then we could over-estimate. Not sure if this happens on average though. Could be always < q-value of optimal action on average. Would be more coherent with fig 2.2. to keep it as the average reward.

- In Exercise 2.7, is it supposed to sum to 1, or did I do a mistake?

- In Figure 2.4, why does Sutton's UCB performs slightly better but my UCB performs way better?

Answer: there was a mistake bc I was using random walk for one and not the other. Then, for big c values (>=2 for instance),  the exploration factor is twice as much as for 1. So when it chooses its first action (which was pretty good), it will need to visit other less good actions (thus decreasing from the just "argmax").
