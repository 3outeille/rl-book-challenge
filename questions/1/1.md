- Why are exploratory moves important if we don't learn from them? It seems that we explore other parts of the tree, but if we don't actually update the values...

Answer: we actually update the state we're exploring (V(e) <- V(e) + \alpha.(V(g*)-V(e)). But not the state that lead to the exploration.

- What are exactly the S_{t+1} and S_t in the update rules? Why does the red arrow goes two steps up?

Answer: because we're estimating the "states before the opponent moves". The opponent is kinda a stochastic environment playing some deterministic thing.
